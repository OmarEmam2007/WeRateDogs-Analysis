{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eb4cf1",
   "metadata": {},
   "source": [
    "# Data Wrangling Report: WeRateDogs Twitter Archive\n",
    "\n",
    "## Introduction\n",
    "This report documents the comprehensive data wrangling process undertaken for the WeRateDogs Twitter archive analysis. The dataset, consisting of tweets from the popular WeRateDogs account, required significant cleaning and restructuring to ensure data quality and usability for analysis.\n",
    "\n",
    "## Data Gathering Process\n",
    "The data was collected from three distinct sources, each requiring different approaches:\n",
    "1. The Twitter archive CSV file, manually downloaded from Udacity\n",
    "2. The image predictions TSV file, programmatically downloaded using Python's requests library\n",
    "3. Additional tweet data obtained through the Twitter API using tweepy\n",
    "\n",
    "## Data Assessment Findings\n",
    "### Quality Issues\n",
    "The initial assessment revealed several quality issues that needed addressing:\n",
    "1. Missing values in critical columns such as ratings and dog names\n",
    "2. Incorrect data types, particularly timestamps stored as strings\n",
    "3. Outliers in numerical columns, especially in rating values\n",
    "4. Duplicated entries across the dataset\n",
    "5. Inconsistent capitalization in dog names\n",
    "6. Invalid or incorrect data in rating denominators\n",
    "7. Incorrect formatting in textual fields\n",
    "8. Inconsistent date entries\n",
    "\n",
    "### Tidiness Issues\n",
    "Two main tidiness issues were identified:\n",
    "1. Multiple variables stored in one column (dog stages)\n",
    "2. One variable stored across multiple datasets\n",
    "\n",
    "## Cleaning Process\n",
    "The cleaning process followed a systematic approach:\n",
    "1. First, I addressed the quality issues one by one, ensuring each change was properly tested\n",
    "2. Then, I tackled the tidiness issues by restructuring the data\n",
    "3. Finally, I merged the datasets into a single, clean master dataset\n",
    "\n",
    "## Challenges Faced\n",
    "Several challenges were encountered during the wrangling process:\n",
    "1. Handling missing values while preserving data integrity\n",
    "2. Standardizing inconsistent data formats\n",
    "3. Ensuring proper merging of datasets without data loss\n",
    "4. Maintaining data quality throughout the cleaning process\n",
    "\n",
    "## Conclusion\n",
    "The wrangling process resulted in a clean, well-structured dataset ready for analysis. The systematic approach to cleaning ensured that all data quality and tidiness issues were properly addressed while maintaining the integrity of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6775d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook wrangle_report.ipynb to html\n",
      "[NbConvertApp] Writing 273926 bytes to wrangle_report.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html wrangle_report.ipynb --output wrangle_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff82ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
